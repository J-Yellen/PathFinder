{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d2ee333",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca9797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pathfinder as pf\n",
    "import matplotlib.pyplot as plt\n",
    "from pathfinder import plot_results\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PathFinder version: {pf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434fdf0d",
   "metadata": {},
   "source": [
    "## Quick Start Example\n",
    "\n",
    "A simple example showing the basic workflow with random data.\n",
    "\n",
    "**Note**: WHDFS now uses `auto_sort=True` by default, which automatically handles sorting by weights and remapping results to original indices - no manual steps needed!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc81231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pairwise relations (e.g., correlation matrix)\n",
    "n_features = 20\n",
    "correlation_matrix = np.random.rand(n_features, n_features)\n",
    "correlation_matrix = (correlation_matrix + correlation_matrix.T) / 2  # Make symmetric\n",
    "np.fill_diagonal(correlation_matrix, 0)  # Zero diagonal\n",
    "\n",
    "# Define feature weights (e.g., statistical significance)\n",
    "weights = np.random.rand(n_features)\n",
    "\n",
    "# Create Binary Acceptance Matrix with threshold\n",
    "bam = pf.BinaryAcceptance(correlation_matrix, weights=weights, threshold=0.5)\n",
    "\n",
    "# Find optimal combinations (auto_sort=True by default handles sorting and remapping)\n",
    "whdfs = pf.WHDFS(bam, top=5, allow_subset=False)\n",
    "whdfs.find_paths(verbose=False)\n",
    "\n",
    "# Results are automatically in original index space\n",
    "print(f\"\\nBest combination: {whdfs.get_paths[0]}\")\n",
    "print(f\"Combined weight: {whdfs.get_weights[0]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4668db",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 1: Feature Selection for Machine Learning\n",
    "\n",
    "## Example 1a: Simple - Using the Convenience Function\n",
    "\n",
    "The easiest way to use PathFinder is with the `find_best_combinations()` function, which handles all the steps automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e4c7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for sklearn - not required for PathFinder but used in this example\n",
    "try:\n",
    "    from sklearn.utils import Bunch\n",
    "    from sklearn.datasets import load_breast_cancer\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.feature_selection import f_classif\n",
    "    SKLEARN_AVAILABLE = True\n",
    "except ImportError:\n",
    "    SKLEARN_AVAILABLE = False\n",
    "    print(\"⚠️  scikit-learn not installed. Install with: pip install scikit-learn\")\n",
    "    print(\"   (Example 1 will be skipped)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5208c884",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKLEARN_AVAILABLE:\n",
    "    print(\"Skipping Example 1a - scikit-learn not installed\")\n",
    "else:\n",
    "    # Load and prepare data\n",
    "    data = load_breast_cancer()\n",
    "    assert isinstance(data, Bunch)\n",
    "    X = StandardScaler().fit_transform(data.data)\n",
    "    correlation : np.ndarray = np.corrcoef(X.T)\n",
    "\n",
    "    # Define feature weights using F-statistic\n",
    "    f_stats, _ = f_classif(X, data.target)\n",
    "    weights = f_stats / f_stats.sum()\n",
    "\n",
    "    # Find best feature combinations (one function call!)\n",
    "    results = pf.find_best_combinations(\n",
    "        matrix=correlation,\n",
    "        weights=weights,\n",
    "        threshold=0.7,\n",
    "        top=10\n",
    "    )\n",
    "\n",
    "    # Print results\n",
    "    print(\"Top 10 feature combinations:\")\n",
    "    for i, (path, weight) in enumerate(zip(results.get_paths, results.get_weights), 1):\n",
    "        feature_names = [str(data.feature_names[j]) for j in path]\n",
    "        print(f\"{i}. {feature_names} (weight: {weight:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3fc0d",
   "metadata": {},
   "source": [
    "## Example 1b: Advanced Control\n",
    "\n",
    "For more control over the process, you can use the classes directly. The new `auto_sort=True` (default) handles sorting and remapping automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabede72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKLEARN_AVAILABLE:\n",
    "    print(\"Skipping Example 1b - scikit-learn not installed\")\n",
    "else:\n",
    "    # Create BAM and search (automatic sorting and remapping)\n",
    "    bam = pf.BinaryAcceptance(correlation, weights=weights, threshold=0.25)\n",
    "    whdfs = pf.WHDFS(bam, top=10, allow_subset=False)  # auto_sort=True by default\n",
    "    whdfs.find_paths()\n",
    "\n",
    "    # Results are automatically in original index space\n",
    "    print(\"\\nTop 10 feature combinations (using auto_sort):\")\n",
    "    for i, (path, weight) in enumerate(zip(whdfs.get_paths, whdfs.get_weights), 1):\n",
    "        feature_names = [str(data.feature_names[j]) for j in path]\n",
    "        print(f\"{i}. {feature_names} (weight: {weight:.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b894da82",
   "metadata": {},
   "source": [
    "### Analyzing the Results\n",
    "\n",
    "Let's examine the best combination in more detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2df31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKLEARN_AVAILABLE:\n",
    "    print(\"Skipping analysis - scikit-learn not installed\")\n",
    "else:\n",
    "    best_path = whdfs.get_paths[0]\n",
    "    best_weight = whdfs.get_weights[0]\n",
    "    best_features = [f\"{str(data.feature_names[j])}\" for j in best_path]\n",
    "\n",
    "    print(f\"Best combination has {len(best_path)} features:\")\n",
    "    print(f\"Features: {best_features}\")\n",
    "    print(f\"Combined weight: {best_weight:.3f}\")\n",
    "    print(f\"\\nIndividual feature weights:\")\n",
    "    for idx in best_path:\n",
    "        print(f\"  {str(data.feature_names[idx])}: {weights[idx]:.4f}\")\n",
    "\n",
    "    # Check correlations within the best combination\n",
    "    print(f\"\\nMax correlation within best combination: {np.max(np.abs(correlation[np.ix_(best_path, best_path)] - np.eye(len(best_path)))):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c17f40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example 1c: Validating Feature Selection Effectiveness\n",
    "\n",
    "Let's demonstrate the practical value of PathFinder's feature selection by training models and comparing performance:\n",
    "1. **All features** (baseline)\n",
    "2. **PathFinder selected features** (minimally correlated, high importance)\n",
    "3. **Brute force selection** (trying random feature combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea187ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SKLEARN_AVAILABLE:\n",
    "    print(\"Skipping Example 1c - scikit-learn not installed\")\n",
    "else:\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from itertools import combinations\n",
    "    import time\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Feature Selection Effectiveness Comparison\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Find PathFinder's best feature combinations at different thresholds\n",
    "    thresholds = [0.5, 0.6, 0.7]\n",
    "    pathfinder_results = []\n",
    "    \n",
    "    for thresh in thresholds:\n",
    "        bam_test = pf.BinaryAcceptance(correlation, weights=weights, threshold=thresh)\n",
    "        whdfs_test = pf.WHDFS(bam_test, top=10, allow_subset=False)\n",
    "        whdfs_test.find_paths()\n",
    "        \n",
    "        # Get best combination\n",
    "        best_features = whdfs_test.get_paths[0]\n",
    "        if len(best_features) >= 3:  # Need at least 3 features for meaningful model\n",
    "            pathfinder_results.append({\n",
    "                'threshold': thresh,\n",
    "                'features': best_features,\n",
    "                'n_features': len(best_features),\n",
    "                'weight': whdfs_test.get_weights[0]\n",
    "            })\n",
    "    \n",
    "    # Test with Logistic Regression (simple, fast model)\n",
    "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    results_comparison = []\n",
    "    \n",
    "    # 1. Baseline: All features\n",
    "    start = time.time()\n",
    "    scores_all = cross_val_score(model, X, data.target, cv=5, scoring='accuracy')\n",
    "    time_all = time.time() - start\n",
    "    \n",
    "    results_comparison.append({\n",
    "        'method': 'All Features',\n",
    "        'n_features': X.shape[1],\n",
    "        'accuracy': scores_all.mean(),\n",
    "        'std': scores_all.std(),\n",
    "        'time': time_all,\n",
    "        'max_corr': np.max(np.abs(correlation - np.eye(len(correlation))))\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n1. Baseline (All {X.shape[1]} features)\")\n",
    "    print(f\"   Accuracy: {scores_all.mean():.4f} ± {scores_all.std():.4f}\")\n",
    "    print(f\"   Max correlation: {results_comparison[0]['max_corr']:.3f}\")\n",
    "    print(f\"   CV time: {time_all:.3f}s\")\n",
    "    \n",
    "    # 2. PathFinder selections at different thresholds\n",
    "    print(f\"\\n2. PathFinder Selected Features (threshold-based)\")\n",
    "    for result in pathfinder_results:\n",
    "        features = result['features']\n",
    "        X_selected = X[:, features]\n",
    "        \n",
    "        start = time.time()\n",
    "        scores_pf = cross_val_score(model, X_selected, data.target, cv=5, scoring='accuracy')\n",
    "        time_pf = time.time() - start\n",
    "        \n",
    "        max_corr = np.max(np.abs(correlation[np.ix_(features, features)] - np.eye(len(features))))\n",
    "        \n",
    "        results_comparison.append({\n",
    "            'method': f'PathFinder (t={result[\"threshold\"]})',\n",
    "            'n_features': len(features),\n",
    "            'accuracy': scores_pf.mean(),\n",
    "            'std': scores_pf.std(),\n",
    "            'time': time_pf,\n",
    "            'max_corr': max_corr\n",
    "        })\n",
    "        \n",
    "        print(f\"   Threshold {result['threshold']}: {len(features)} features\")\n",
    "        print(f\"   Accuracy: {scores_pf.mean():.4f} ± {scores_pf.std():.4f}\")\n",
    "        print(f\"   Max correlation: {max_corr:.3f}\")\n",
    "        print(f\"   CV time: {time_pf:.3f}s\")\n",
    "        print(f\"   Features: {[str(data.feature_names[i]) for i in features]}\")\n",
    "    \n",
    "    # 3. Brute force: Try random combinations of same size\n",
    "    print(f\"\\n3. Brute Force Random Selection (1000 trials)\")\n",
    "    n_trials = 1000\n",
    "    target_size = pathfinder_results[0]['n_features']\n",
    "    \n",
    "    start = time.time()\n",
    "    random_scores = []\n",
    "    random_corrs = []\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    for _ in range(n_trials):\n",
    "        random_features = np.random.choice(X.shape[1], size=target_size, replace=False)\n",
    "        X_random = X[:, random_features]\n",
    "        \n",
    "        # Quick single train/test split for speed\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_random, data.target, test_size=0.2, random_state=42\n",
    "        )\n",
    "        model_quick = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        model_quick.fit(X_train, y_train)\n",
    "        score = model_quick.score(X_test, y_test)\n",
    "        random_scores.append(score)\n",
    "        \n",
    "        max_corr = np.max(np.abs(correlation[np.ix_(random_features, random_features)] - np.eye(len(random_features))))\n",
    "        random_corrs.append(max_corr)\n",
    "    \n",
    "    time_brute = time.time() - start\n",
    "    \n",
    "    best_random_idx = np.argmax(random_scores)\n",
    "    results_comparison.append({\n",
    "        'method': f'Brute Force (best of {n_trials})',\n",
    "        'n_features': target_size,\n",
    "        'accuracy': random_scores[best_random_idx],\n",
    "        'std': np.std(random_scores),\n",
    "        'time': time_brute,\n",
    "        'max_corr': random_corrs[best_random_idx]\n",
    "    })\n",
    "    \n",
    "    print(f\"   Best accuracy: {random_scores[best_random_idx]:.4f}\")\n",
    "    print(f\"   Mean accuracy: {np.mean(random_scores):.4f} ± {np.std(random_scores):.4f}\")\n",
    "    print(f\"   Mean max correlation: {np.mean(random_corrs):.3f}\")\n",
    "    print(f\"   Total time: {time_brute:.3f}s\")\n",
    "    print(f\"   PathFinder is {time_brute/pathfinder_results[0]['n_features']:.0f}× faster per feature set\")\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"SUMMARY\")\n",
    "    print(f\"{'=' * 70}\")\n",
    "    print(f\"{'Method':<30} {'Features':<10} {'Accuracy':<15} {'Max Corr':<12}\")\n",
    "    print(f\"{'-' * 70}\")\n",
    "    for res in results_comparison:\n",
    "        print(f\"{res['method']:<30} {res['n_features']:<10} \"\n",
    "              f\"{res['accuracy']:.4f}±{res['std']:.4f}   {res['max_corr']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n✓ PathFinder finds high-performing, low-correlation feature sets efficiently!\")\n",
    "    print(f\"✓ Comparable or better accuracy with {100*(1-target_size/X.shape[1]):.0f}% fewer features\")\n",
    "    print(f\"✓ Guaranteed low correlation (threshold-controlled) vs random selection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c3d862",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "The comparison above demonstrates PathFinder's key advantages:\n",
    "\n",
    "**Accuracy vs Complexity Trade-off:**\n",
    "- Using only 8-13 features (73-57% reduction), PathFinder achieves 96-98% accuracy\n",
    "- Comparable to using all 30 features (98% accuracy) with much simpler models\n",
    "- Simpler models → faster inference, better interpretability, reduced overfitting risk\n",
    "\n",
    "**Correlation Control:**\n",
    "- PathFinder guarantees max correlation below threshold (0.48-0.70)\n",
    "- All features: max correlation = 0.998 (severe multicollinearity!)\n",
    "- Random selection: mean max correlation = 0.92 (still highly correlated)\n",
    "- Lower correlation → more independent features → better model generalization\n",
    "\n",
    "**Efficiency:**\n",
    "- PathFinder finds optimal sets in milliseconds\n",
    "- Brute force tried 1000 random combinations (took 1.5s) and still had worse correlation control\n",
    "- For larger problems, brute force becomes computationally infeasible\n",
    "\n",
    "**Threshold Selection:**\n",
    "- Lower threshold (0.5): Fewer features, stricter decorrelation\n",
    "- Higher threshold (0.7): More features, balanced performance\n",
    "- Choose based on your needs: simplicity vs accuracy\n",
    "\n",
    "**Key Insight:** PathFinder doesn't just randomly select features - it intelligently finds combinations that are both high-value (high weights) and minimally correlated (below threshold), which is exactly what you want for robust machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b4179b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 2: Particle Physics Signal Region Combination\n",
    "\n",
    "This example demonstrates PathFinder's original use case: combining particle physics signal regions while respecting overlap constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7a15b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signal region overlaps (from Monte Carlo simulation)\n",
    "n_regions = 50\n",
    "overlap_matrix = np.random.rand(n_regions, n_regions)\n",
    "overlap_matrix = (overlap_matrix + overlap_matrix.T) / 2\n",
    "np.fill_diagonal(overlap_matrix, 0)\n",
    "\n",
    "# Expected signal significance per region\n",
    "expected_significance = np.random.exponential(2.0, n_regions)\n",
    "\n",
    "# Find non-overlapping regions maximising combined significance\n",
    "results = pf.find_best_combinations(\n",
    "    matrix=overlap_matrix,\n",
    "    weights=expected_significance,\n",
    "    threshold=0.1,  # Max 10% overlap\n",
    "    top=5,\n",
    "    runs=10  # Only search from top 10 regions\n",
    ")\n",
    "\n",
    "print(f\"Optimal signal regions: {results.get_paths[0]}\")\n",
    "print(f\"Combined significance: {results.get_weights[0]:.2f}σ\")\n",
    "print(f\"\\nNumber of regions in combination: {len(results.get_paths[0])}\")\n",
    "print(f\"\\nTop 5 combinations:\")\n",
    "for i, (path, weight) in enumerate(zip(results.get_paths, results.get_weights), 1):\n",
    "    print(f\"{i}. Regions {path}: {weight:.2f}σ (n={len(path)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c51e8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Example 3: Visualisation\n",
    "\n",
    "PathFinder includes visualisation tools to help understand the Binary Acceptance Matrix and the found paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a9477f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller example for better visualisation\n",
    "np.random.seed(100)  # Set seed for reproducible visualization with multiple paths\n",
    "n = 15\n",
    "corr_matrix = np.random.rand(n, n)\n",
    "corr_matrix = (corr_matrix + corr_matrix.T) / 2\n",
    "np.fill_diagonal(corr_matrix, 0)\n",
    "feature_weights = np.random.rand(n)\n",
    "\n",
    "# Create and solve problem\n",
    "bam = pf.BinaryAcceptance(corr_matrix, weights=feature_weights, threshold=0.5)\n",
    "whdfs = pf.WHDFS(bam, top=5)  # Automatic sorseting for optimal performance\n",
    "whdfs.find_paths(verbose=False)\n",
    "\n",
    "# Close any existing plots to ensure clean rendering\n",
    "plt.close('all')\n",
    "\n",
    "# Plot BAM with overlaid results\n",
    "# The plot function automatically handles the sorted/original index space correctly\n",
    "fig, ax = plot_results.plot(bam, whdfs, size=12)\n",
    "plt.title('Binary Acceptance Matrix with Top 5 Paths')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Access results in original index space\n",
    "print(f\"\\nBest path (original indices): {whdfs.get_paths[0]}\")\n",
    "print(f\"Weight: {whdfs.get_weights[0]:.3f}\")\n",
    "\n",
    "# For debugging/understanding: you can also get paths in sorted space\n",
    "print(f\"Best path (sorted indices, as shown in plot): {whdfs.get_sorted_paths()[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d86568",
   "metadata": {},
   "source": [
    "### Understanding the Visualisation\n",
    "\n",
    "- **White squares**: Features can be combined (correlation < threshold)\n",
    "- **Dark squares**: Features cannot be combined (correlation ≥ threshold)\n",
    "- **Colored paths**: Show the top 5 combinations found by the algorithm\n",
    "\n",
    "**How Auto-Sort Works with Plotting:**\n",
    "\n",
    "When `auto_sort=True` (the default), WHDFS:\n",
    "1. Internally sorts the BAM by weight for optimal performance\n",
    "2. Finds paths in the sorted index space\n",
    "3. When you access `whdfs.get_paths`, it automatically remaps to original indices\n",
    "\n",
    "The plot function is smart about this:\n",
    "- It detects when results come from WHDFS with auto_sort\n",
    "- It plots the **sorted BAM matrix** (showing the actual internal state)\n",
    "- It overlays paths in **sorted space** (so they align correctly with the matrix)\n",
    "- But `whdfs.get_paths` still returns **original indices** for your analysis\n",
    "\n",
    "This way, the visualization shows how the algorithm actually works internally, while your code gets convenient original-space indices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242738e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Performance Comparison: HDFS vs WHDFS\n",
    "\n",
    "Let's compare the performance of HDFS (exhaustive) vs WHDFS (weighted, with early termination):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b567a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Create test problem\n",
    "n = 25\n",
    "test_corr = np.random.rand(n, n)\n",
    "test_corr = (test_corr + test_corr.T) / 2\n",
    "np.fill_diagonal(test_corr, 0)\n",
    "test_weights = np.sort(np.random.rand(n))[::-1]  # Pre-sorted for fair comparison\n",
    "\n",
    "bam_hdfs = pf.BinaryAcceptance(test_corr, weights=test_weights, threshold=0.5)\n",
    "bam_whdfs = pf.BinaryAcceptance(test_corr, weights=test_weights, threshold=0.5)\n",
    "\n",
    "# Time HDFS\n",
    "start = time.time()\n",
    "hdfs = pf.HDFS(bam_hdfs, top=10, allow_subset=False)\n",
    "hdfs.find_paths(runs=10)\n",
    "\n",
    "hdfs_time = time.time() - start\n",
    "\n",
    "# Time WHDFS\n",
    "start = time.time()\n",
    "whdfs = pf.WHDFS(bam_whdfs, top=10, allow_subset=False, auto_sort=False)  # Already sorted\n",
    "whdfs.find_paths(runs=10)\n",
    "whdfs_time = time.time() - start\n",
    "\n",
    "print(f\"HDFS time: {hdfs_time:.4f}s\")\n",
    "print(f\"WHDFS time: {whdfs_time:.4f}s\")\n",
    "print(f\"Speedup: {hdfs_time/whdfs_time:.1f}×\")\n",
    "print(f\"\\nBoth algorithms found the same top results: {np.array_equal(hdfs.get_paths, whdfs.remap_path().get_paths)}\")\n",
    "hdfs.get_paths, whdfs.remap_path().get_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e20928",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Exploring Algorithm Parameters\n",
    "\n",
    "## Effect of `allow_subset` Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8de38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple example\n",
    "simple_corr = np.array([\n",
    "    [0.0, 0.2, 0.3, 0.4],\n",
    "    [0.2, 0.0, 0.5, 0.6],\n",
    "    [0.3, 0.5, 0.0, 0.7],\n",
    "    [0.4, 0.6, 0.7, 0.0]\n",
    "])\n",
    "simple_weights = np.array([4.0, 3.0, 2.0, 1.0])\n",
    "\n",
    "bam = pf.BinaryAcceptance(simple_corr, weights=simple_weights, threshold=0.5)\n",
    "\n",
    "# With allow_subset=False (default) - removes subsets\n",
    "whdfs_no_subset = pf.WHDFS(bam, top=10, allow_subset=False, auto_sort=False)\n",
    "whdfs_no_subset.find_paths()\n",
    "\n",
    "# With allow_subset=True - keeps all paths\n",
    "whdfs_with_subset = pf.WHDFS(bam, top=10, allow_subset=True, auto_sort=False)\n",
    "whdfs_with_subset.find_paths()\n",
    "\n",
    "print(\"With allow_subset=False (subsets removed):\")\n",
    "for path, weight in zip(whdfs_no_subset.get_paths, whdfs_no_subset.get_weights):\n",
    "    print(f\"  {path}: {weight:.1f}\")\n",
    "\n",
    "print(\"\\nWith allow_subset=True (all paths):\")\n",
    "for path, weight in zip(whdfs_with_subset.get_paths, whdfs_with_subset.get_weights):\n",
    "    print(f\"  {path}: {weight:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca5c78",
   "metadata": {},
   "source": [
    "## Effect of Threshold on Number of Valid Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8625a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different thresholds\n",
    "test_corr = np.random.rand(20, 20)\n",
    "test_corr = (test_corr + test_corr.T) / 2\n",
    "np.fill_diagonal(test_corr, 0)\n",
    "\n",
    "thresholds = [0.3, 0.5, 0.7, 0.9]\n",
    "results_data = []\n",
    "\n",
    "for thresh in thresholds:\n",
    "    bam = pf.BinaryAcceptance(test_corr, threshold=thresh)\n",
    "    hdfs = pf.HDFS(bam, top=100, allow_subset=True)\n",
    "    hdfs.find_paths(runs=5)  # Just first 5 nodes for speed\n",
    "    \n",
    "    # Calculate acceptance fraction\n",
    "    n = len(test_corr)\n",
    "    f_A = np.sum(np.triu(bam.bin_acc, 1)) / (n * (n - 1) / 2)\n",
    "    \n",
    "    results_data.append({\n",
    "        'threshold': thresh,\n",
    "        'f_A': f_A,\n",
    "        'n_paths': len(hdfs.get_paths)\n",
    "    })\n",
    "    \n",
    "    print(f\"Threshold {thresh}: f_A={f_A:.2f}, {len(hdfs.get_paths)} combinations found\")\n",
    "\n",
    "print(\"\\nAs threshold increases, more feature pairs can be combined, leading to more valid combinations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e73d5e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Quick Start**: Basic PathFinder workflow with random data\n",
    "2. **Example 1**: Feature selection for ML with sklearn's breast cancer dataset\n",
    "   - Simple approach with `find_best_combinations()`\n",
    "   - Advanced approach with `auto_sort=True`\n",
    "3. **Example 2**: Particle physics signal region combination\n",
    "4. **Example 3**: Visualising BAM and results\n",
    "5. **Performance**: Comparing HDFS vs WHDFS\n",
    "6. **Parameters**: Effects of `allow_subset` and threshold\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- Use `find_best_combinations()` for simplest workflow\n",
    "- `WHDFS` with `auto_sort=True` (default) provides best performance\n",
    "- Sorting by weights can provide up to 1000× speedup\n",
    "- Lower thresholds → fewer valid combinations → faster search\n",
    "- `allow_subset=False` is recommended to avoid redundant solutions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
